{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stress = pd.read_csv('./Dataset/stress.csv', index_col='collect_time', parse_dates=['collect_time'])\n",
    "Targets = pd.read_csv('./Dataset/targets.csv', index_col='collect_time', parse_dates=['collect_time'])\n",
    "print(Stress.shape)\n",
    "print(Targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "# To test the model\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_train, X_test, y_train, y_test = train_test_split(Stress,Targets, test_size=0.2, shuffle=True, random_state=7)\n",
    "#################################################################################################\n",
    "# Feature scaling required for neural network\n",
    "\n",
    "scaler_x.fit(X_train)\n",
    "scaled_X_train = scaler_x.transform(X_train)\n",
    "scaled_X_test = scaler_x.transform(X_test)\n",
    "\n",
    "\n",
    "scaler_y.fit(y_train)\n",
    "scaled_y_train = scaler_y.transform(y_train)\n",
    "scaled_y_test = scaler_y.transform(y_test)\n",
    "#################################################################################################\n",
    "np.random.seed(7)\n",
    "scoring_param = make_scorer(mean_squared_error,greater_is_better=False)\n",
    "#################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "lin_reg = LinearRegression().fit(scaled_X_train, scaled_y_train)\n",
    "t1 = time()\n",
    "Time_Taken = (t1-t0)\n",
    "print(\"Time taken to train the model: %0.2f\" % Time_Taken,\"seconds\")\n",
    "Y_Train_Pred = lin_reg.predict(scaled_X_train)\n",
    "Y_Train_Pred = scaler_y.inverse_transform(Y_Train_Pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_train,Y_Train_Pred))\n",
    "RSQ = r2_score(y_train,Y_Train_Pred)\n",
    "MAE = mean_absolute_error(y_train, Y_Train_Pred)\n",
    "MAPE = mean_absolute_percentage_error(y_train, Y_Train_Pred) * 100\n",
    "################################################################\n",
    "################################################################\n",
    "print(\"############ Model Accuracy on Daily Training Data ############\")\n",
    "print(\"RMSE: %0.4f\" % rmse)\n",
    "print(\"R-squared: %0.4f\" % RSQ)\n",
    "print(\"MAPE: {:.2f}\".format(MAPE))\n",
    "print(\"########################################\")\n",
    "Y_Test_Pred = lin_reg.predict(scaled_X_test)\n",
    "Y_Test_Pred = scaler_y.inverse_transform(Y_Test_Pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test,Y_Test_Pred))\n",
    "RSQ = r2_score(y_test,Y_Test_Pred)\n",
    "MAE = mean_absolute_error(y_test, Y_Test_Pred)\n",
    "MAPE = mean_absolute_percentage_error(y_test, Y_Test_Pred) * 100\n",
    "################################################################\n",
    "################################################################\n",
    "print(\"############ Model Accuracy on Daily Testing Data ############\")\n",
    "print(\"RMSE: %0.4f\" % rmse)\n",
    "print(\"R-squared: %0.4f\" % RSQ)\n",
    "print(\"MBE: %0.4f\" % MAE)\n",
    "print(\"MAPE: {:.2f}\".format(MAPE))\n",
    "print(\"########################################\")\n",
    "################################################################\n",
    "################################################################\n",
    "# 保存模型\n",
    "with open('./Modelpkl/LR.pkl','wb') as f:\n",
    "    pickle.dump(lin_reg, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "p_grid = dict(n_estimators = [int(i) for i in np.linspace(100,2000,num=20)],\n",
    "    max_depth = [int(i) for i in np.linspace(1,10,num=10)],\n",
    "  min_samples_leaf = [int(i) for i in np.linspace(1,10,num=10)])\n",
    "\n",
    "rf = GridSearchCV(estimator = RandomForestRegressor(random_state=7), param_grid = p_grid, \n",
    "                     scoring = scoring_param, cv = 4, verbose=1, n_jobs=-1)\n",
    "\n",
    "rf.fit(scaled_X_train ,scaled_y_train)\n",
    "t1 = time()\n",
    "Time_Taken = (t1-t0)\n",
    "print(\"Time taken to train the model: %0.2f\" % Time_Taken,\"seconds\")\n",
    "print(\"Best RF Estimators: %0.3f\" % rf.best_params_.get('n_estimators'))\n",
    "print(\"Best RF Max Depth: %0.3f\" % rf.best_params_.get('max_depth'))\n",
    "print(\"Best RF Min Samples in Leaf: %0.3f\" % rf.best_params_.get('min_samples_leaf'))\n",
    "Y_Train_Pred = rf.predict(scaled_X_train)\n",
    "Y_Train_Pred = scaler_y.inverse_transform(Y_Train_Pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_train,Y_Train_Pred))\n",
    "RSQ = r2_score(y_train,Y_Train_Pred)\n",
    "MAE = mean_absolute_error(y_train, Y_Train_Pred)\n",
    "MAPE = mean_absolute_percentage_error(y_train, Y_Train_Pred) * 100\n",
    "################################################################\n",
    "################################################################\n",
    "print(\"############ Model Accuracy on Daily Training Data ############\")\n",
    "print(\"RMSE: {:.4f}\".format(rmse))\n",
    "print(\"R-squared: {:.4f}\".format(RSQ))\n",
    "print(\"MAE: {:.4f}\".format(MAE))\n",
    "print(\"MAPE: {:.2f}\".format(MAPE))\n",
    "print(\"########################################\")\n",
    "Y_Test_Pred = rf.predict(scaled_X_test)\n",
    "Y_Test_Pred = scaler_y.inverse_transform(Y_Test_Pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test,Y_Test_Pred))\n",
    "RSQ = r2_score(y_test,Y_Test_Pred)\n",
    "MAE = mean_absolute_error(y_test, Y_Test_Pred)\n",
    "MAPE = mean_absolute_percentage_error(y_test, Y_Test_Pred)\n",
    "################################################################\n",
    "################################################################\n",
    "print(\"############ Model Accuracy on Daily Testing Data ############\")\n",
    "print(\"RMSE: {:.4f}\".format(rmse))\n",
    "print(\"R-squared: {:.4f}\".format(RSQ))\n",
    "print(\"MAE: {:.4f}\".format(MAE))\n",
    "print(\"MAPE: {:.2f}\".format(MAPE))\n",
    "print(\"########################################\")\n",
    "print(\" \")\n",
    "# 保存模型\n",
    "with open('./Modelpkl/RF.pkl','wb') as f:\n",
    "    pickle.dump(rf.best_estimator_, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "p_grid = dict(n_estimators = [int(i) for i in np.linspace(100,2000,num=20)],\n",
    "    max_depth = [int(i) for i in np.linspace(1,10,num=10)],\n",
    "  learning_rate = np.linspace(0.001,0.1,num=10))\n",
    "\n",
    "xgb = GridSearchCV(estimator = XGBRegressor(random_state=7), param_grid = p_grid, \n",
    "                     scoring = scoring_param, cv = 4, verbose=1, n_jobs=-1)\n",
    "xgb.fit(scaled_X_train,scaled_y_train)\n",
    "t1 = time()\n",
    "Time_Taken = (t1-t0)\n",
    "print(\"Time taken to train the model: %0.2f\" % Time_Taken,\"seconds\")\n",
    "print(\"Best XGB Estimators: %0.3f\" % xgb.best_params_.get('n_estimators'))\n",
    "print(\"Best XGB Max Depth: %0.3f\" % xgb.best_params_.get('max_depth'))\n",
    "print(\"Best XGB Learning Rate: %0.3f\" % xgb.best_params_.get('learning_rate'))\n",
    "Y_Train_Pred = xgb.predict(scaled_X_train)\n",
    "Y_Train_Pred = scaler_y.inverse_transform(Y_Train_Pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_train,Y_Train_Pred))\n",
    "RSQ = r2_score(y_train,Y_Train_Pred)\n",
    "MAE = mean_absolute_error(y_train, Y_Train_Pred)\n",
    "MAPE = mean_absolute_percentage_error(y_train, Y_Train_Pred) * 100\n",
    "################################################################\n",
    "################################################################\n",
    "print(\"############ Model Accuracy on Daily Training Data ############\")\n",
    "print(\"RMSE: %0.4f\" % rmse)\n",
    "print(\"R-squared: %0.4f\" % RSQ)\n",
    "print('MAE: {:.4f}'.format(MAE))\n",
    "print('MAPE: {:.2f}'.format(MAPE))\n",
    "print(\"########################################\")\n",
    "Y_Test_Pred = xgb.predict(scaled_X_test)\n",
    "Y_Test_Pred = scaler_y.inverse_transform(Y_Test_Pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test,Y_Test_Pred))\n",
    "RSQ = r2_score(y_test,Y_Test_Pred)\n",
    "MAE = mean_absolute_error(y_test, Y_Test_Pred)\n",
    "MAPE = mean_absolute_percentage_error(y_test, Y_Test_Pred)\n",
    "################################################################\n",
    "################################################################\n",
    "print(\"############ Model Accuracy on Daily Testing Data ############\")\n",
    "print(\"RMSE: %0.4f\" % rmse)\n",
    "print(\"R-squared: %0.4f\" % RSQ)\n",
    "print('MAE: {:.4f}'.format(MAE))\n",
    "print('MAPE: {:.2f}'.format(MAPE))\n",
    "print(\"########################################\")\n",
    "# 保存模型\n",
    "with open('./Modelpkl/xgb.pkl','wb') as f:\n",
    "    pickle.dump(xgb.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "x_data = pd.read_csv('./Dataset/stress.csv', index_col='collect_time', parse_dates=['collect_time'])\n",
    "y_data = pd.read_csv('./Dataset/targets.csv', index_col='collect_time', parse_dates=['collect_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "import gc\n",
    "import logging\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "warnings.filterwarnings('ignore')\n",
    "cols = x_data.columns\n",
    "raw_data = pd.merge(x_data, y_data, left_index=True, right_index=True)\n",
    "target = y_data.columns\n",
    "depth = 64\n",
    "batch_size = 128\n",
    "prediction_horizon = 1\n",
    "L = len(x_data)\n",
    "train_size = int(0.6*L)\n",
    "val_size = int(0.2*L)\n",
    "test_size = L - train_size - val_size\n",
    "\n",
    "data_train = raw_data.iloc[:train_size + val_size]\n",
    "data_test = raw_data.iloc[train_size+val_size:]\n",
    "\n",
    "scaler_cols = MinMaxScaler(feature_range=(0, 1)).fit(data_train[cols].values)\n",
    "data_train_cols_scale = scaler_cols.transform(data_train[cols].values)\n",
    "data_test_cols_scale = scaler_cols.transform(data_test[cols].values)\n",
    "data_train_cols_scale = pd.DataFrame(data_train_cols_scale)\n",
    "data_test_cols_scale = pd.DataFrame(data_test_cols_scale)\n",
    "\n",
    "scaler_target = MinMaxScaler(feature_range=(\n",
    "    0, 1)).fit(data_train[target].values)\n",
    "data_train_target_scale = scaler_target.transform(data_train[target].values)\n",
    "data_test_target_scale = scaler_target.transform(data_test[target].values)\n",
    "data_train_target_scale = pd.DataFrame(data_train_target_scale)\n",
    "data_test_target_scale = pd.DataFrame(data_test_target_scale)\n",
    "\n",
    "# train\n",
    "X1 = np.zeros((train_size + val_size, depth, len(cols)))\n",
    "y_his1 = np.zeros((train_size + val_size, depth, 7))\n",
    "y1 = np.zeros((train_size + val_size, 7))\n",
    "\n",
    "for i, name in enumerate(data_train_cols_scale.columns):\n",
    "    for j in range(depth):\n",
    "        X1[:, j, i] = data_train_cols_scale[name].shift(\n",
    "            depth - j - 1).fillna(method=\"bfill\")\n",
    "for j in range(depth):\n",
    "    y_his1[:, j, :] = data_train_target_scale.shift(\n",
    "        depth - j - 1).fillna(method=\"bfill\")\n",
    "y1 = data_train_target_scale.shift(- depth -\n",
    "                                   prediction_horizon+1).fillna(method=\"bfill\")\n",
    "\n",
    "X_train = X1[depth-1:-prediction_horizon]\n",
    "y_his_train = y_his1[depth-1:-prediction_horizon]\n",
    "y_train = y1[:-depth-prediction_horizon+1]\n",
    "\n",
    "del X1, y1, y_his1, data_train_cols_scale, data_train_target_scale\n",
    "gc.collect()\n",
    "\n",
    "# test\n",
    "X3 = np.zeros((test_size, depth, len(cols)))\n",
    "y_his3 = np.zeros((test_size, depth, 7))\n",
    "y3 = np.zeros((test_size, 7))\n",
    "\n",
    "for i, name in enumerate(data_test_cols_scale.columns):\n",
    "    for j in range(depth):\n",
    "        X3[:, j, i] = data_test_cols_scale[name].shift(\n",
    "            depth - j - 1).fillna(method=\"bfill\")\n",
    "for j in range(depth):\n",
    "    y_his3[:, j, :] = data_test_target_scale.shift(\n",
    "        depth - j - 1).fillna(method=\"bfill\")\n",
    "y3 = data_test_target_scale.shift(- depth -\n",
    "                                  prediction_horizon+1).fillna(method=\"bfill\")\n",
    "\n",
    "X_test = X3[depth-1:-prediction_horizon]\n",
    "y_his_test = y_his3[depth-1:-prediction_horizon]\n",
    "y_test = y3[:-depth-prediction_horizon+1]\n",
    "\n",
    "del X3, y3, y_his3, data_test_cols_scale, data_test_target_scale\n",
    "gc.collect()\n",
    "# X_train 自变量，y_his历史预测值，y_tain预测值对应的真实值\n",
    "X_train_t = torch.Tensor(X_train)\n",
    "X_test_t = torch.Tensor(X_test)\n",
    "y_his_train_t = torch.Tensor(y_his_train)\n",
    "y_his_test_t = torch.Tensor(y_his_test)\n",
    "y_train_t = torch.Tensor(y_train.values)\n",
    "y_test_t = torch.Tensor(y_test.values)\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "X_train = np.concatenate((X_train, y_his_train), axis=2).reshape(X_train.shape[0], -1) \n",
    "y_train = y_train.values\n",
    "\n",
    "X_test = np.concatenate((X_test, y_his_test), axis=2).reshape(X_test.shape[0], -1) \n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testLoader(X, y,  input_size, batch_size):\n",
    "    X = X.reshape(X.shape[0], -1, input_size + 7)\n",
    "    X_test = X[:, :, :input_size]\n",
    "    y_his_test = X[:, :, input_size:]\n",
    "    del X\n",
    "\n",
    "    X_test_t = torch.Tensor(X_test)\n",
    "    y_his_test_t = torch.Tensor(y_his_test)\n",
    "    y_test_t = torch.Tensor(y)\n",
    "\n",
    "    del  X_test, y_his_test, y\n",
    "\n",
    "    test_loader = DataLoader(TensorDataset(\n",
    "        X_test_t, y_his_test_t, y_test_t), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    del X_test_t, y_his_test_t, y_test_t\n",
    "    gc.collect()\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "def trainValLoader(X, y, input_size, batch_size):\n",
    "    X = X.reshape(X.shape[0], -1, input_size + 7)\n",
    "    train_size = int(X.shape[0] * 0.75)\n",
    "\n",
    "    X_train = X[:, :, :input_size]\n",
    "    X_val = X[train_size:, :, :input_size]\n",
    "\n",
    "    y_his_train = X[:, :, input_size:]\n",
    "    y_his_val = X[train_size:, :, input_size:]\n",
    "\n",
    "    y_train = y[:, :]\n",
    "    y_val = y[train_size:, :]\n",
    "\n",
    "    del X, y\n",
    "\n",
    "    X_train_t = torch.Tensor(X_train)\n",
    "    X_val_t = torch.Tensor(X_val)\n",
    "    y_his_train_t = torch.Tensor(y_his_train)\n",
    "    y_his_val_t = torch.Tensor(y_his_val)\n",
    "    y_train_t = torch.Tensor(y_train)\n",
    "    y_val_t = torch.Tensor(y_val)\n",
    "\n",
    "    del X_train,X_val, y_his_train,y_his_val, y_train, y_val\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(\n",
    "        X_train_t, y_his_train_t, y_train_t), shuffle=True, batch_size=batch_size)\n",
    "    val_loader = DataLoader(TensorDataset(\n",
    "        X_val_t, y_his_val_t, y_val_t), shuffle=True, batch_size=batch_size)\n",
    "    del  X_train_t, y_his_train_t, y_train_t, X_val_t, y_his_val_t, y_val_t\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import clone\n",
    "from networks.LSTMModel import Module_LSTM\n",
    "import time\n",
    "def Base_fit(model, train_len, val_len, train_loader, val_loader, device):\n",
    "    epochs = 100\n",
    "    min_val_loss = 9999\n",
    "    loss_function = nn.MSELoss().to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    epoch_scheduler = torch.optim.lr_scheduler.StepLR(opt, 20, gamma=0.9)\n",
    "    train_start = time.monotonic()\n",
    "    for i in range(epochs):\n",
    "        mse_train = 0\n",
    "        model.train()\n",
    "        for batch_x, batch_y_h, batch_y in train_loader:\n",
    "            opt.zero_grad()\n",
    "            y_pred = model(batch_x.to(device), batch_y_h.to(device))\n",
    "            loss = loss_function(y_pred, batch_y.to(device))\n",
    "            loss.backward()\n",
    "            mse_train += loss.item() * batch_x.shape[0]\n",
    "            opt.step()\n",
    "        epoch_scheduler.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            mse_val = 0\n",
    "            for batch_x, batch_y_h, batch_y in val_loader:\n",
    "                output = model(batch_x.to(device), batch_y_h.to(device))\n",
    "                mse_val += loss_function(output, batch_y.to(device)).item() * batch_x.shape[0]\n",
    "        if min_val_loss > mse_val ** 0.5:\n",
    "            min_val_loss = mse_val ** 0.5\n",
    "        if i % 10 == 0:\n",
    "            logging.info(\"Iter: \" + str(i) + \" train: \" + str((mse_train / train_len) ** 0.5) + \" val: \" + str(\n",
    "                (mse_val / val_len) ** 0.5))\n",
    "    train_end = time.monotonic()\n",
    "    logging.info(\"LSTM training time: {:.4f}\".format(train_end - train_start))\n",
    "    return min_val_loss\n",
    "\n",
    "def cross_verification(base_estimator, parameters, model_name, X_train, y_train, device):\n",
    "    skfolds = KFold(n_splits=4)\n",
    "    fit_funcs ={\n",
    "        'LSTM': Base_fit,\n",
    "        'GRU': Base_fit,\n",
    "    }\n",
    "    local_loss = []\n",
    "    for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "        estimator = clone(base_estimator)\n",
    "        cloned_parameters = {}\n",
    "        for k, v in parameters.items():\n",
    "            cloned_parameters[k] = clone(v, safe=False)\n",
    "        estimator = clone(estimator.set_params(**cloned_parameters))\n",
    "        estimator.to(device)   \n",
    "        X_train_folds = X_train[train_index]\n",
    "        y_train_folds = (y_train[train_index])\n",
    "        X_test_fold = X_train[test_index]\n",
    "        y_test_fold = (y_train[test_index])\n",
    "        train_len = X_train_folds.shape[0]\n",
    "        val_len = X_test_fold.shape[0]\n",
    "\n",
    "        X_train_folds = X_train_folds.reshape(X_train_folds.shape[0], -1, 38 + 7)\n",
    "        X_test_fold = X_test_fold.reshape(X_test_fold.shape[0], -1, 38 + 7)\n",
    "        \n",
    "        X_train_t = torch.Tensor(X_train_folds[:,:,:38])\n",
    "        X_val_t = torch.Tensor(X_test_fold[:,:,:38])\n",
    "        y_his_train_t = torch.Tensor(X_train_folds[:,:,38:])\n",
    "        y_his_val_t = torch.Tensor(X_test_fold[:,:,38:])\n",
    "        y_train_t = torch.Tensor(y_train_folds)\n",
    "        y_val_t = torch.Tensor(y_test_fold)\n",
    "        \n",
    "        del X_train_folds,X_test_fold,y_train_folds,y_test_fold\n",
    "        # del X_train, y_his_train, y_train\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(\n",
    "            X_train_t, y_his_train_t, y_train_t), shuffle=True, batch_size=128)\n",
    "        val_loader = DataLoader(TensorDataset(\n",
    "            X_val_t, y_his_val_t, y_val_t), shuffle=True, batch_size=128)\n",
    "        \n",
    "        del X_train_t, y_his_train_t, y_train_t, X_val_t, y_his_val_t, y_val_t\n",
    "        gc.collect()\n",
    "\n",
    "        local_min_val_loss = fit_funcs[model_name](estimator, train_len, val_len, train_loader, val_loader, device)\n",
    "        local_loss.append(local_min_val_loss)\n",
    "        del estimator\n",
    "        gc.collect()\n",
    "    return np.mean(local_loss) \n",
    "\n",
    "def get_best_params(estimator, param_grid,model_name, X_train, y_train, device):\n",
    "    global_val_loss = []\n",
    "    candidate_params = list(ParameterGrid(param_grid))\n",
    "    base_estimator = clone(estimator)\n",
    "    for cand_idx, parameters in enumerate(candidate_params):\n",
    "        logging.info('cuurent parameters index: {}'.format(cand_idx))\n",
    "        local_val_loss = cross_verification(base_estimator, parameters, model_name, X_train, y_train, device)\n",
    "        global_val_loss.append(local_val_loss)\n",
    "        torch.cuda.empty_cache()\n",
    "    for cand_idx, parameters in enumerate(candidate_params):\n",
    "        logging.info('val loss: {}'.format(global_val_loss[cand_idx]))\n",
    "    index = np.argmin(global_val_loss)\n",
    "    best_parameters = candidate_params[index]\n",
    "    return best_parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.LSTMModel import Module_LSTM\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score, mean_squared_error\n",
    "import warnings\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "warnings.filterwarnings('ignore')\n",
    "filename = \"./logfiles/lstm.log\"\n",
    "logging.basicConfig(filename=filename, format='%(asctime)s %(filename)s %(levelname)s %(message)s',\n",
    "                    datefmt='%a %d %b %Y %H:%M:%S', filemode='w', level=logging.INFO, force=True)\n",
    "train_loader, val_loader = trainValLoader(X_train, y_train, 38, 128)\n",
    "test_loader = testLoader(X_test, y_test, 38, 128)\n",
    "param_grid = {\n",
    "    'hidden_size':[16,32,64,128],\n",
    "    'dropout': [ i * 0.1 for i in range(1, 5)],\n",
    "}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_estimator = Module_LSTM().to(device)\n",
    "best_params = get_best_params(base_estimator, param_grid, 'LSTM', X_train, y_train, device)\n",
    "logging.info('best params: hidden_size: {}, dropout: {}'.format(best_params['hidden_size'], best_params['dropout']))\n",
    "\n",
    "model = clone(clone(base_estimator).set_params(**best_params))\n",
    "model.to(device)\n",
    "loss_function = nn.MSELoss().to(device)\n",
    "min_val_loss = 9999\n",
    "epochs = 100\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epoch_scheduler = torch.optim.lr_scheduler.StepLR(opt, 20, gamma=0.9)\n",
    "train_start = time.monotonic()\n",
    "for i in range(epochs):\n",
    "    mse_train = 0\n",
    "    model.train()\n",
    "    for batch_x, batch_y_h, batch_y in train_loader:\n",
    "        opt.zero_grad()\n",
    "        y_pred = model(batch_x.to(device), batch_y_h.to(device))\n",
    "        loss = loss_function(y_pred, batch_y.to(device))\n",
    "        loss.backward()\n",
    "        mse_train += loss.item() * batch_x.shape[0]\n",
    "        opt.step()\n",
    "    epoch_scheduler.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mse_val = 0\n",
    "        for batch_x, batch_y_h, batch_y in val_loader:\n",
    "            output = model(batch_x.to(device), batch_y_h.to(device))\n",
    "            mse_val += loss_function(output, batch_y.to(device)).item() * batch_x.shape[0]\n",
    "    if min_val_loss > mse_val ** 0.5:\n",
    "        min_val_loss = mse_val ** 0.5\n",
    "        filename = \"./Modelpkl/lstm.pt\"\n",
    "        torch.save(model.state_dict(), filename)\n",
    "    if i % 10 == 0:\n",
    "        logging.info(\"Iter: \" + str(i) + \" train: \" + str((mse_train / (len(X_train) * 0.75)) ** 0.5) + \" val: \" + str(\n",
    "            (mse_val / (len(X_train) * 0.25)) ** 0.5))\n",
    "train_end = time.monotonic()\n",
    "logging.info(\"LSTM training time: {:.4f}\".format(train_end - train_start))\n",
    "\n",
    "# train set metrics\n",
    "filename = \"./Modelpkl/lstm.pt\"\n",
    "model.load_state_dict(torch.load(filename))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mse_val = 0\n",
    "    preds = []\n",
    "    true = []\n",
    "    for batch_x, batch_y_h, batch_y in train_loader:\n",
    "        output = model(batch_x.to(device), batch_y_h.to(device))\n",
    "        preds.append(output.detach().cpu().numpy())\n",
    "        true.append(batch_y.detach().cpu().numpy())\n",
    "        l = loss_function(output, batch_y.to(device)).item()\n",
    "        mse_val += l\n",
    "preds = np.concatenate(preds)\n",
    "true = np.concatenate(true)\n",
    "\n",
    "true = scaler_target.inverse_transform(true)\n",
    "preds = scaler_target.inverse_transform(preds)\n",
    "mse = mean_squared_error(true, preds)\n",
    "mae = mean_absolute_error(true, preds)\n",
    "mape = mean_absolute_percentage_error(true, preds) * 100\n",
    "r2 = r2_score(true, preds)\n",
    "\n",
    "logging.info('Train data result:')\n",
    "logging.info('Train RMSE: {:.4f}, Train R2: {:.4f}, Train MAE: {:.4f}, Train MAPE: {:.4f}'.format(\n",
    "    mse**0.5, r2, mae, mape))\n",
    "\n",
    "# test set metrics\n",
    "filename = \"./Modelpkl/lstm.pt\"\n",
    "model.load_state_dict(torch.load(filename))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mse_val = 0\n",
    "    preds = []\n",
    "    true = []\n",
    "    for batch_x, batch_y_h, batch_y in test_loader:\n",
    "        output = model(batch_x.to(device), batch_y_h.to(device))\n",
    "        preds.append(output.detach().cpu().numpy())\n",
    "        true.append(batch_y.detach().cpu().numpy())\n",
    "        l = loss_function(output, batch_y.to(device)).item()\n",
    "        mse_val += l\n",
    "preds = np.concatenate(preds)\n",
    "true = np.concatenate(true)\n",
    "\n",
    "true = scaler_target.inverse_transform(true)\n",
    "preds = scaler_target.inverse_transform(preds)\n",
    "mse = mean_squared_error(true, preds)\n",
    "mae = mean_absolute_error(true, preds)\n",
    "mape = mean_absolute_percentage_error(true, preds) * 100\n",
    "r2 = r2_score(true, preds)\n",
    "logging.info('Test data result:')\n",
    "logging.info('Test RMSE: {:.4f}, Test R2: {:.4f}, Test MAE: {:.4f}, Test MAPE: {:.4f}'.format(\n",
    "    mse**0.5, r2, mae, mape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.GRUModel import Module_GRU\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score, mean_squared_error\n",
    "import warnings\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "warnings.filterwarnings('ignore')\n",
    "filename = \"./logfiles/gru.log\"\n",
    "logging.basicConfig(filename=filename, format='%(asctime)s %(filename)s %(levelname)s %(message)s',\n",
    "                    datefmt='%a %d %b %Y %H:%M:%S', filemode='w', level=logging.INFO, force=True)\n",
    "train_loader, val_loader = trainValLoader(X_train, y_train, 38, 128)\n",
    "test_loder = testLoader(X_test, y_test, 38, 128)\n",
    "param_grid = {\n",
    "    'hidden_size':[16,32,64,128],\n",
    "    'dropout': [ i * 0.1 for i in range(1, 5)],\n",
    "}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_estimator = Module_GRU().to(device)\n",
    "best_params = get_best_params(base_estimator, param_grid, 'GRU', X_train, y_train, device)\n",
    "logging.info('best params: hidden_size: {}, dropout: {}'.format(best_params['hidden_size'], best_params['dropout']))\n",
    "\n",
    "model = clone(clone(base_estimator).set_params(**best_params))\n",
    "model.to(device)\n",
    "loss_function = nn.MSELoss().to(device)\n",
    "min_val_loss = 9999\n",
    "epochs = 100\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epoch_scheduler = torch.optim.lr_scheduler.StepLR(opt, 20, gamma=0.9)\n",
    "train_start = time.monotonic()\n",
    "for i in range(epochs):\n",
    "    mse_train = 0\n",
    "    model.train()\n",
    "    for batch_x, batch_y_h, batch_y in train_loader:\n",
    "        opt.zero_grad()\n",
    "        y_pred = model(batch_x.to(device), batch_y_h.to(device))\n",
    "        loss = loss_function(y_pred, batch_y.to(device))\n",
    "        loss.backward()\n",
    "        mse_train += loss.item() * batch_x.shape[0]\n",
    "        opt.step()\n",
    "    epoch_scheduler.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mse_val = 0\n",
    "        for batch_x, batch_y_h, batch_y in val_loader:\n",
    "            output = model(batch_x.to(device), batch_y_h.to(device))\n",
    "            mse_val += loss_function(output, batch_y.to(device)).item() * batch_x.shape[0]\n",
    "    if min_val_loss > mse_val ** 0.5:\n",
    "        min_val_loss = mse_val ** 0.5\n",
    "        filename = \"./save/gru.pt\"\n",
    "        torch.save(model.state_dict(), filename)\n",
    "    if i % 10 == 0:\n",
    "        logging.info(\"Iter: \" + str(i) + \" train: \" + str((mse_train / (len(X_train) * 0.75)) ** 0.5) + \" val: \" + str(\n",
    "            (mse_val / (len(X_train) * 0.25)) ** 0.5))\n",
    "train_end = time.monotonic()\n",
    "logging.info(\"GRU training time: {:.4f}\".format(train_end - train_start))\n",
    "\n",
    "# train set metrics\n",
    "filename = \"./save/gru.pt\"\n",
    "model.load_state_dict(torch.load(filename))\n",
    "model.eval()\n",
    "print(model)\n",
    "with torch.no_grad():\n",
    "    mse_val = 0\n",
    "    preds = []\n",
    "    true = []\n",
    "    for batch_x, batch_y_h, batch_y in train_loader:\n",
    "        output = model(batch_x.to(device), batch_y_h.to(device))\n",
    "        preds.append(output.detach().cpu().numpy())\n",
    "        true.append(batch_y.detach().cpu().numpy())\n",
    "        l = loss_function(output, batch_y.to(device)).item()\n",
    "        mse_val += l\n",
    "preds = np.concatenate(preds)\n",
    "true = np.concatenate(true)\n",
    "\n",
    "true = scaler_target.inverse_transform(true)\n",
    "preds = scaler_target.inverse_transform(preds)\n",
    "mse = mean_squared_error(true, preds)\n",
    "mae = mean_absolute_error(true, preds)\n",
    "mape = mean_absolute_percentage_error(true, preds) * 100\n",
    "r2 = r2_score(true, preds)\n",
    "\n",
    "logging.info('Train data result:')\n",
    "logging.info('Train RMSE: {:.4f}, Train R2: {:.4f}, Train MAE: {:.4f}, Train MAPE: {:.4f}'.format(\n",
    "    mse**0.5, r2, mae, mape))\n",
    "\n",
    "# test set metrics\n",
    "filename = \"./save/gru.pt\"\n",
    "model.load_state_dict(torch.load(filename))\n",
    "model.eval()\n",
    "print(model)\n",
    "with torch.no_grad():\n",
    "    mse_val = 0\n",
    "    preds = []\n",
    "    true = []\n",
    "    for batch_x, batch_y_h, batch_y in test_loader:\n",
    "        output = model(batch_x.to(device), batch_y_h.to(device))\n",
    "        preds.append(output.detach().cpu().numpy())\n",
    "        true.append(batch_y.detach().cpu().numpy())\n",
    "        l = loss_function(output, batch_y.to(device)).item()\n",
    "        mse_val += l\n",
    "preds = np.concatenate(preds)\n",
    "true = np.concatenate(true)\n",
    "\n",
    "true = scaler_target.inverse_transform(true)\n",
    "preds = scaler_target.inverse_transform(preds)\n",
    "mse = mean_squared_error(true, preds)\n",
    "mae = mean_absolute_error(true, preds)\n",
    "mape = mean_absolute_percentage_error(true, preds) * 100\n",
    "r2 = r2_score(true, preds)\n",
    "logging.info('Test data result:')\n",
    "logging.info('Test RMSE: {:.4f}, Test R2: {:.4f}, Test MAE: {:.4f}, Test MAPE: {:.4f}'.format(\n",
    "    mse**0.5, r2, mae, mape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
